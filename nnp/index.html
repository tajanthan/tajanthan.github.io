<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>NNP</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../assets/stylesheets/normalize.css" media="screen">
    <!--<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>-->
    <link rel="stylesheet" type="text/css" href="../assets/stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../assets/stylesheets/github-light.css" media="screen">
	<link rel="shortcut icon" href="../images/snip.png" />
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Neural Network Pruning</h1>
	  <!--<h2 class="project-tagline">
      <strong>Thalaiyasingam Ajanthan<sup>1</sup>, Namhoon Lee<sup>2</sup>, Zhiwei Xu<sup>1</sup>, Vibhav Vineet<sup>3</sup>, Martin Jaggi<sup>4</sup>, Stephen Gould<sup>1</sup>, Philip H. S. Torr<sup>2</sup>, and Richard Hartley<sup>1</sup></strong>
	  </h2>
	  <h2 class="project-tagline">
	  <sup>1</sup> Australian National University, <sup>2</sup> University of Oxford, <sup>3</sup> Microsoft Research, <sup>4</sup>EPFL</h2>-->
    </section>

    <section class="main-content">
      

<h2 id="abstract">Abstract</h2>
<div>
Neural network pruning is a prominent technique to compress a neural network by reducing the number of parameters (or the number of neurons) which is often desirable for resource-limitted devices and/or real-time applications. In this project, we develop new pruning methods specifically targetting <em>pruning at initialization</em> and attempt to theoretically understand the training of sparse networks as well as evaluate the scalability of such pruning at initialization methods. 
</div>

<h2 id="publications">Publications</h2>
<div>Understanding the Effects of Data Parallelism and Sparsity on Neural Network Training.<br />
Namhoon Lee, <strong>Thalaiyasingam Ajanthan</strong>, Philip H. S. Torr, and Martin Jaggi.<br />
International Conference on Learning Representations (ICLR), May 2021.<br/>
[<a href="docs/dataparallelism.pdf" title="pdf">pdf</a>] 
<!--    [<a href="docs/lpdensecrf_supp.pdf" title="supp">supplementary</a>]-->
[<a href="https://arxiv.org/abs/2003.11316" title="arxiv">arxiv</a>] 
<!--[<a href="https://iclr.cc/virtual_2020/poster_HJeTo2VFwH.html" title="talk">talk</a>]
[<a href="https://github.com/namhoonlee/spp-public" title="code">code</a>]-->
[<a href="" class="bibLink">bib</a>] 
<pre class="bibBlock" style="display: none;">
@article{lee_dataparallelism_iclr21,
  author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS and Jaggi, Martin},
  title = {Understanding the Effects of Data Parallelism and Sparsity on Neural Network Training},
  journal = {ICLR},
  year = {2021}
}</pre>
</div></br>

<div>
RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs.<br />
Zhiwei Xu, <strong>Thalaiyasingam Ajanthan</strong>, Vibhav Vineet, and Richard Hartley.<br />
International Conference on 3D Vision (3DV), November 2020. <font color="darkorange">(oral, best student paper)</font><br/>
[<a href="docs/ranp.pdf" title="pdf">pdf</a>] 
[<a href="https://arxiv.org/abs/2010.02488" title="arxiv">arxiv</a>]
[<a href="https://www.youtube.com/watch?v=KadydhOJnBE&t=71s&ab_channel=RabbitDoResearch" title="talk">talk</a>]
[<a href="https://github.com/zwxu064/RANP" title="code">code</a>]
[<a href="" class="bibLink">bib</a>]
<pre class="bibBlock" style="display: none;">
@article{xu_ranp_3dv20,
  author = {Xu, Zhiwei, and Ajanthan, Thalaiyasingam, and Vineet, Vibhav, and Hartley, Richard},
  title = {RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs},
  journal = {3DV},
  year = {2020}
}</pre>
</div></br>

<div>A Signal Propagation Perspective for Pruning Neural Networks at Initialization.<br />
Namhoon Lee, <strong>Thalaiyasingam Ajanthan</strong>, Stephen Gould, and Philip H. S. Torr.<br />
International Conference on Learning Representations (ICLR), April 2020. <font color="darkorange">(spotlight)</font><br/>
[<a href="docs/disnip.pdf" title="pdf">pdf</a>] 
<!--    [<a href="docs/lpdensecrf_supp.pdf" title="supp">supplementary</a>]-->
[<a href="https://arxiv.org/abs/1906.06307" title="arxiv">arxiv</a>] 
[<a href="https://iclr.cc/virtual_2020/poster_HJeTo2VFwH.html" title="talk">talk</a>]
[<a href="https://github.com/namhoonlee/spp-public" title="code">code</a>]
[<a href="" class="bibLink">bib</a>] 
<pre class="bibBlock" style="display: none;">
@article{lee_disnip_iclr20,
  author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Gould, Stephen and Torr, Philip HS},
  title = {A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
  journal = {ICLR},
  year = {2020}
}</pre>
</div></br>

<div>SNIP: Single-shot Network Pruning based on Connection Sensitivity.<br />
Namhoon Lee, <strong>Thalaiyasingam Ajanthan</strong>, and Philip H. S. Torr.<br />
International Conference on Learning Representations (ICLR), May 2019. <br/>
[<a href="docs/snip.pdf" title="pdf">pdf</a>] 
<!--    [<a href="docs/lpdensecrf_supp.pdf" title="supp">supplementary</a>]-->
[<a href="https://arxiv.org/abs/1810.02340" title="arxiv">arxiv</a>] 
[<a href="docs/snip-poster.pdf" title="poster">poster</a>]
[<a href="http://www.robots.ox.ac.uk/~namhoon/doc/slides-snip-msr.pdf" title="talk">talk</a>]
[<a href="https://github.com/namhoonlee/snip-public" title="code">code</a>]
[<a href="" class="bibLink">bib</a>]
<pre class="bibBlock" style="display: none;">
@article{lee_snip_iclr19,
  author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  title = {{SNIP:} Single-shot Network Pruning based on Connection Sensitivity},
  journal = {ICLR},
  year = {2019}
}</pre>
</div>


      <footer class="site-footer">
          <span class="site-footer-owner">
            <a href="../index.html#research">
                User profile</a>.</span>
      </footer>

    </section>
	<script src="../assets/js/jquery.min.js"></script>
	<script src="../assets/js/bib.js" type="text/javascript"></script>
  
  </body>
</html>
