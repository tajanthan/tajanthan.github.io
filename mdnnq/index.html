<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>MDNNQ</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">MDNNQ</h1>
      <h2 class="project-tagline"><strong>Mirror Descent View for Neural Network Quantization</strong></br>
      Thalaiyasingam Ajanthan*, Kartik Gupta*, Philip H. S. Torr, Richard Hartley, and Puneet K. Dokania</h2>
      <a href="docs/mdnnq.pdf" class="btn">pdf</a>
      <!--<a href="docs/memf_poster.pdf" class="btn">poster</a>-->
      <!-- <a href="https://github.com/tajanthan/pmf" class="btn">code</a>-->
    </section>

    <section class="main-content">
      

<h2 id="abstract">Abstract</h2>
<p>
Quantizing large Neural Networks (NN) while maintaining the performance is highly desirable for resource-limited devices due to reduced memory and time complexity. NN quantization is usually formulated as a constrained optimization problem and optimized via a modified version of gradient descent. In this work, by interpreting the continuous parameters (unconstrained) as the dual of the quantized ones , we introduce a Mirror Descent (MD) framework (Bubeck (2015)) for NN quantization. Specifically, we provide conditions on the projections (i.e., mapping from continuous to quantized ones) which would enable us to derive valid mirror maps and in turn the respective MD updates. Furthermore, we discuss a numerically stable implementation of MD by storing an additional set of auxiliary dual variables (continuous). This update is strikingly analogous to the popular Straight Through Estimator (STE) based method which is typically viewed as a "trick" to avoid vanishing gradients issue but here we show that it is an implementation method for MD for certain projections. Our experiments on standard classification datasets (CIFAR-10/100, TinyImageNet) with convolutional and residual architectures show that our MD variants obtain fully-quantized networks with accuracies very close to the floating-point networks.
</p>
<h2 id="publication">Publication</h2>
<p>Mirror Descent View for Neural Network Quantization.<br />
<strong>Thalaiyasingam Ajanthan</strong>*, Kartik Gupta*, Philip H. S. Torr, Richard Hartley, and Puneet K. Dokania.<br />
Arxiv Preprint: arXiv:1910.08237, October 2019.</p>
<p>[<a href="docs/mdnnq.pdf" title="pdf">pdf</a>] 
<!--  [<a href="docs/pmf-supp.pdf" title="supp">supplementary</a>]-->
[<a href="https://arxiv.org/abs/1910.08237" title="arxiv">arxiv</a>]
<!--[<a href="https://github.com/tajanthan/pmf" title="code">code</a>]-->
</p>

      <footer class="site-footer">
          <span class="site-footer-owner">
            <a href="../index.html#research">
                User profile</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
