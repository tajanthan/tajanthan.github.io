<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>RWalk by tajanthan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">RWalk</h1>
      <h2 class="project-tagline"><strong>Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence</strong></br>
      Arslan Chaudhry*, Puneet K. Dokania*, Thalaiyasingam Ajanthan*, and Philip H.S. Torr</h2>
      <!--<a href="docs/lpdensecrf.pdf" class="btn">pdf</a>-->
      <!--<a href="docs/memf_poster.pdf" class="btn">poster</a>-->
      <!--<a href="https://github.com/oval-group/DenseCRF" class="btn">code</a>-->
    </section>

    <section class="main-content">
      

<h2 id="abstract">Abstract</h2>
<p>We study the incremental learning problem for the classification task, a key component in developing life-long learning systems. The main challenges while learning in an incremental manner are to preserve and update the knowledge of the model. In this work, we propose a generalization of Path Integral (Zenke et al., 2017) and EWC (Kirkpatrick et al., 2016) with a theoretically grounded KL-divergence based perspective. We show that, to preserve and update the knowledge, regularizing the model's likelihood distribution is more intuitive and provides better insights to the problem. To do so, we use KL-divergence as a measure of distance which is equivalent to computing distance in a Riemannian manifold induced by the Fisher information matrix. Furthermore, to enhance the learning flexibility, the regularization is weighted by a parameter importance score that is calculated along the entire training trajectory. Contrary to forgetting, as the algorithm progresses, the regularized loss makes the network intransigent, resulting in its inability to discriminate new tasks from the old ones. We show that this problem of intransigence can be addressed by storing a small subset of representative samples from previous datasets. In addition, in order to evaluate the performance of an incremental learning algorithm, we introduce two novel metrics to evaluate forgetting and intransigence. Experimental evaluation on incremental version of MNIST and CIFAR-100 classification datasets shows that our approach outperforms existing state-of-the-art baselines in all the evaluation metrics.</p>
<h2 id="publication">Publication</h2>
<p>Riemannian Walk for Incremental Learning: Understanding Forgetting and Intransigence.<br />
Arslan Chaudhry*, Puneet K. Dokania*, <strong>Thalaiyasingam Ajanthan</strong>*, and Philip H.S. Torr.<br />
Arxiv Preprint: arXiv:1801.10112, January 2018.</p>
<p><!--[<a href="docs/lpdensecrf.pdf" title="pdf">pdf</a>] 
    [<a href="docs/lpdensecrf_supp.pdf" title="supp">supplementary</a>]-->
[<a href="https://arxiv.org/abs/1801.10112" title="arxiv">arxiv</a>] 
<!--[<a href="https://github.com/oval-group/DenseCRF" title="code">code</a>] -->

      <footer class="site-footer">
          <p>* Equal contribution.</p>
          <span class="site-footer-owner">
            <a href="https://tajanthan.github.io/#research">
                User profile</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
